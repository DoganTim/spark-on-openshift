NAME: spark-history-server
LAST DEPLOYED: Fri Jun 11 17:14:10 2021
NAMESPACE: spark-operator
STATUS: pending-install
REVISION: 1
TEST SUITE: None
HOOKS:
MANIFEST:
---
# Source: spark-history-server/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: spark-history-server
  labels:
    app.kubernetes.io/name: spark-history-server
    helm.sh/chart: spark-history-server-1.5.0
    app.kubernetes.io/instance: spark-history-server
    app.kubernetes.io/managed-by: Helm
---
# Source: spark-history-server/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-history-server
  labels:
    app.kubernetes.io/name: spark-history-server
    helm.sh/chart: spark-history-server-1.5.0
    app.kubernetes.io/instance: spark-history-server
    app.kubernetes.io/managed-by: Helm
data:
  enablePVC: "false"
  enableGCS: "false"
  enableS3: "true"
  enableWASBS: "false"
  accessKeyName: "aws-access-key"
  enableIAM: "true"
  endpoint: "s3.openshift-storage.svc:443"
  logDirectory: "s3a://spark-history-server-e8a3f03a-4fb0-4a37-b5c1-9949e2ebc8a3/"
  secret: "spark-history-server"
  secretKeyName: "aws-secret-key"
---
# Source: spark-history-server/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: spark-history-server-cr
  labels:
    app.kubernetes.io/name: spark-history-server
    helm.sh/chart: spark-history-server-1.5.0
    app.kubernetes.io/instance: spark-history-server
    app.kubernetes.io/managed-by: Helm
rules:
- apiGroups: [""]
  resources: ["deployments", "pods"]
  verbs: ["*"]
---
# Source: spark-history-server/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: spark-history-server-crb
subjects:
- kind: ServiceAccount
  name: spark-history-server
  namespace: spark-operator
roleRef:
  kind: ClusterRole
  name: spark-history-server-cr
  apiGroup: rbac.authorization.k8s.io
---
# Source: spark-history-server/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: spark-history-server
  labels:
    app.kubernetes.io/name: spark-history-server
    helm.sh/chart: spark-history-server-1.5.0
    app.kubernetes.io/instance: spark-history-server
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
  - port: 18080
    targetPort: historyport
    protocol: TCP
    name: http-historyport
  selector:
    app.kubernetes.io/name: spark-history-server
    app.kubernetes.io/instance: spark-history-server
---
# Source: spark-history-server/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spark-history-server
  labels:
    app.kubernetes.io/name: spark-history-server
    helm.sh/chart: spark-history-server-1.5.0
    app.kubernetes.io/instance: spark-history-server
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: spark-history-server
      app.kubernetes.io/instance: spark-history-server
  template:
    metadata:
      labels:
        app.kubernetes.io/name: spark-history-server
        app.kubernetes.io/instance: spark-history-server
    spec:
      serviceAccountName: spark-history-server
      containers:
      - name: spark-history-server
        image: "gcr.io/spark-operator/spark-operator:v1beta2-1.2.3-3.1.1"
        imagePullPolicy: IfNotPresent
        env:
        - name: HADOOP_CONF_DIR
          value: /etc/hadoop
        - name: SPARK_NO_DAEMONIZE
          value: "true"
        ports:
        - name: historyport
          containerPort: 18080
          protocol: TCP
        resources:
          {}
        command:
        - "/bin/sh"
        - "-c"
        - >
          if [ "$enablePVC" == "true" ]; then
            export SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS \
            -Dspark.history.fs.logDirectory=file:/data/$eventsDir";
          elif [ "$enableGCS" == "true" ]; then
            export SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS \
            -Dspark.history.fs.logDirectory=$logDirectory";
            if [ "$enableIAM" == "false" ]; then
              export SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS \
              -Dspark.hadoop.google.cloud.auth.service.account.json.keyfile=/etc/secrets/$key";
            fi;
          elif [ "$enableS3" == "true" ]; then
            export SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS \
              -Dspark.history.fs.logDirectory=$logDirectory \
              -Dspark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem";
            if [ -n "$endpoint" ] && [ "$endpoint" != "default" ]; then
              export SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS \
              -Dspark.hadoop.fs.s3a.endpoint=$endpoint";
            fi;
            if [ "$enableIAM" == "false" ]; then
              export SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS \
              -Dspark.hadoop.fs.s3a.access.key=$(cat /etc/secrets/${accessKeyName}) \
              -Dspark.hadoop.fs.s3a.secret.key=$(cat /etc/secrets/${secretKeyName})";
            fi;
          elif [ "$enableWASBS" == "true" ]; then
            container=$(cat /etc/secrets/${containerKeyName})
            storageAccount=$(cat /etc/secrets/${storageAccountNameKeyName})

            export SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS \
              -Dspark.history.fs.logDirectory=$logDirectory \
              -Dspark.hadoop.fs.defaultFS=wasbs://$container@$storageAccount.blob.core.windows.net \
              -Dspark.hadoop.fs.wasbs.impl=org.apache.hadoop.fs.azure.NativeAzureFileSystem \
              -Dspark.hadoop.fs.AbstractFileSystem.wasbs.impl=org.apache.hadoop.fs.azure.Wasbs";
            if [ "$sasKeyMode" == "true" ]; then
              export SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS \
                -Dspark.hadoop.fs.azure.local.sas.key.mode=true \
                -Dspark.hadoop.fs.azure.sas.$container.$storageAccount.blob.core.windows.net=$(cat /etc/secrets/${sasKeyName})";
            else
              export SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS \
                -Dspark.hadoop.fs.azure.account.key.$storageAccount.blob.core.windows.net=$(cat /etc/secrets/${storageAccountKeyName})";
            fi;
          else
            export SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS \
            -Dspark.history.fs.logDirectory=$logDirectory";
          fi;
          /opt/spark/bin/spark-class org.apache.spark.deploy.history.HistoryServer;
        envFrom:
        - configMapRef:
            name: spark-history-server
        livenessProbe:
          httpGet:
            path: /
            port: historyport
        readinessProbe:
          httpGet:
            path: /
            port: historyport

NOTES:
Get the application URL by running the following commands. Note that the UI would take a minute or two to show up after the pods and services are ready.
  export POD_NAME=$(kubectl get pods --namespace spark-operator -l "app.kubernetes.io/instance=spark-history-server" -o jsonpath="{.items[0].metadata.name}")
    Note: If on OpenShift, change to the chart namespace before running the command below.'
  kubectl port-forward $POD_NAME 8080:18080
    Now visit http://127.0.0.1:8080 to use your application.'
